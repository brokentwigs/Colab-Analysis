{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brokentwigs/Colab-Analysis/blob/main/DecisionTrees_vs_RandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsTJKc74e-Uw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "0339ca35-f3fb-485a-dc5d-26ac33658df1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0c58b9a9c850>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_column_transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#feature selection algorithms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# visualizatoin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# data wrangling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# data preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.compose import make_column_transformer\n",
        "import category_encoders as ce\n",
        "\n",
        "#feature selection algorithms\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "\n",
        "\n",
        "#machine learning algorithms\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold, KFold, RandomizedSearchCV,GridSearchCV\n",
        "import xgboost\n",
        "\n",
        "#optimization\n",
        "from numpy import sqrt\n",
        "from numpy import argmax\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxq26TlPpDgs"
      },
      "outputs": [],
      "source": [
        "!pip install category_encoders\n",
        "#!pip install catboost\n",
        "#!pip install scikit-optimize i\n",
        "\n",
        "#run these pip installations first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t53bRCuLipks"
      },
      "outputs": [],
      "source": [
        "# description\n",
        "description = pd.read_csv('/content/WiDS_Datathon_2020_Dictionary.csv')\n",
        "description_dict = description.set_index('Variable Name').to_dict(orient='index')\n",
        "# data\n",
        "df = pd.read_csv('/content/training_v2.csv')\n",
        "\n",
        "#print_full(description)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhAxCTHOo3ZS"
      },
      "outputs": [],
      "source": [
        "# Returns category description\n",
        "\n",
        "def category(description, string):\n",
        "\n",
        "    return description[description.Category==string]\n",
        "\n",
        "# Returns highly correlated features\n",
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()  # Set of all the names of correlated columns\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
        "                colname = corr_matrix.columns[i]  # getting the name of column\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJaZbd4ym8qY"
      },
      "outputs": [],
      "source": [
        "# Parameter definitions\n",
        "test_size = 0.2 # proportion for train versus test+val split\n",
        "val_size = 0.5 # proportion for test versus val split\n",
        "random_state = 42  # random state is used to set a seed for randomness, which is only relevant for reproducibility purposes\n",
        "max_missing = 0.8  # maximum percentage of missing values for a column to be dropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86LZy6AFm-4x"
      },
      "outputs": [],
      "source": [
        "X = df.copy().drop(['hospital_death'], axis=1)\n",
        "y = df['hospital_death'].copy()\n",
        "y_apache = df['apache_4a_hospital_death_prob'].copy() #save APACHE scores for later evaluation on train / test / validation data\n",
        "\n",
        "\n",
        "\"\"\" SPLIT DATA SET \"\"\"\n",
        "# split the dataset into train and test+validation set\n",
        "(\n",
        "    X_train,\n",
        "    X_test,\n",
        "    y_train,\n",
        "    y_test,\n",
        "    y_apache_train,\n",
        "    y_apache_test,\n",
        "    ) = train_test_split(X, y, y_apache,\n",
        "                         test_size=test_size, # used for testing and validation\n",
        "                         random_state=random_state # for reproducibility\n",
        "                        )\n",
        "# split the test set into test + validation set\n",
        "(\n",
        "    X_val,\n",
        "    X_test,\n",
        "    y_val,\n",
        "    y_test,\n",
        "    y_apache_val,\n",
        "    y_apache_test,\n",
        "    ) = train_test_split(X_test, y_test, y_apache_test,\n",
        "                         test_size=val_size, # used for testing and validation\n",
        "                         random_state=random_state # for reproducibility\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgFWizFDoLFv"
      },
      "outputs": [],
      "source": [
        "apache_cov = category(description,\"APACHE covariate\")\n",
        "vitals = category(description,\"vitals\")\n",
        "labs = category(description,\"labs\")\n",
        "\n",
        "corr_features_apache_cov = correlation(X_train[apache_cov[\"Variable Name\"]], 0.85)\n",
        "corr_features_vitals = correlation(X_train[vitals[\"Variable Name\"]], 0.85)\n",
        "corr_features_labs = correlation(X_train[labs[\"Variable Name\"]], 0.85)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYkDNGqMo-P7"
      },
      "outputs": [],
      "source": [
        "# DROP THESE COLUMNS\n",
        "to_drop = set()\n",
        "to_drop.update(['apache_2_bodysystem', 'apache_4a_icu_death_prob', 'encounter_id', 'patient_id', 'weight', 'height',\n",
        "                'icu_stay_type', 'readmission_status', 'hospital_id', 'icu_id',\n",
        "                'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob'])\n",
        "to_drop.update(corr_features_apache_cov)\n",
        "to_drop.update(corr_features_vitals)\n",
        "to_drop.update(corr_features_labs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K68zQNkpHB-"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.drop(to_drop, axis=1)\n",
        "X_val = X_val.drop(to_drop, axis=1)\n",
        "X_test = X_test.drop(to_drop, axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhhV6Ox2paM-"
      },
      "outputs": [],
      "source": [
        "\"\"\"MISSING VALUES\"\"\"\n",
        "# drop columns with many missing values\n",
        "missing = X_train.isna().sum() > max_missing * len(X_train)\n",
        "missing = missing[missing].index\n",
        "X_train = X_train.drop(missing, axis=1)\n",
        "X_val = X_val.drop(missing, axis=1)\n",
        "X_test = X_test.drop(missing, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpmdNZuYpPxi"
      },
      "outputs": [],
      "source": [
        "def customprocessor(X, y):\n",
        "    num = [col for col in X.columns if X[col].dtypes != 'O' and ~X[col].isin([0.0,1.0,np.nan]).all()]\n",
        "    cat = [col for col in X.columns if X[col].dtypes == 'O' or X[col].isin([0.0,1.0,np.nan]).all()]\n",
        "\n",
        "    # Imputing numerical values\n",
        "    linearreg = LinearRegression()\n",
        "    imp = IterativeImputer(estimator=linearreg, missing_values=np.nan, max_iter=5, imputation_order='roman', verbose=2, random_state=42)\n",
        "    X[num] = imp.fit_transform(X[num])\n",
        "\n",
        "    # Imputing categorical values\n",
        "    impcat = SimpleImputer(strategy=\"most_frequent\")\n",
        "    X[cat] = impcat.fit_transform(X[cat])\n",
        "\n",
        "    # Turn 'gender' column into a binary column\n",
        "    gender = {'M': 1,'F': 0}\n",
        "    X.gender = [gender[item] for item in X.gender]\n",
        "\n",
        "    # Target encoding with additive smoothing\n",
        "    def calc_smooth_mean(X, y, by, on, m):\n",
        "        df = pd.concat([X, y], axis=1)\n",
        "\n",
        "        # Compute the global mean\n",
        "        mean = df[on].mean()\n",
        "\n",
        "        # Compute the number of values and the mean of each group\n",
        "        agg = df.groupby(by)[on].agg(['count', 'mean'])\n",
        "        counts = agg['count']\n",
        "        means = agg['mean']\n",
        "\n",
        "        # Compute the \"smoothed\" means\n",
        "        smooth = (counts * means + m * mean) / (counts + m)\n",
        "\n",
        "        # Replace each value by the according smoothed mean\n",
        "        return X[by].map(smooth)\n",
        "\n",
        "    X['ethnicity'] = calc_smooth_mean(X, y, by='ethnicity', on='hospital_death', m=10)\n",
        "    X['hospital_admit_source'] = calc_smooth_mean(X, y, by='hospital_admit_source', on='hospital_death', m=10)\n",
        "    X['icu_admit_source'] = calc_smooth_mean(X, y, by='icu_admit_source', on='hospital_death', m=10)\n",
        "    X['icu_type'] = calc_smooth_mean(X, y, by='icu_type', on='hospital_death', m=10)\n",
        "    X['apache_3j_bodysystem'] = calc_smooth_mean(X, y, by='apache_3j_bodysystem', on='hospital_death', m=10)\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzOqlYBnpiyr"
      },
      "outputs": [],
      "source": [
        "# Applying pre-processing steps\n",
        "X_train = customprocessor(X_train, y_train)\n",
        "X_val = customprocessor(X_val, y_val)\n",
        "X_test = customprocessor(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fnhscFaqULo"
      },
      "outputs": [],
      "source": [
        "#Candidate algorithms. Describe the machine learning algorithms you will try (e.g., logistic regression, decision tree, random forest, XGBoost). For each algorithm, describe possible\n",
        "#advantages and disadvantages, given the problem requirements.\n",
        "#Model Selection. Use the algorithm(s) to train machine learning models (one for each al- gorithm).\n",
        "#Describe your model selection pipeline, including the model selection procedure\n",
        "#(hyperparameter tuning procedure, cross-validation, etc.) and the evaluation metric(s) that you use during model selection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpeU4P7HA3fw"
      },
      "outputs": [],
      "source": [
        "def random_search(model, X, Y, n_splits=10, n_repeats=3):\n",
        "\n",
        "    \"\"\"\n",
        "  Random search for hyperparameter tuning:\n",
        "  model - Random Forest classifier, DecisionTrees\n",
        "  X - Data matrix X, typically training data\n",
        "  Y - Target vector Y, typically training labels/binary\n",
        "  n_splits - splits to be made in repeated stratified kfold cv\n",
        "  n_repeats - number of times to repeat stratified kfold cv\n",
        "  Note --> The Search spaces are preset/defined randomly\n",
        "      \"\"\"\n",
        "    if type(model) == type(RandomForestClassifier()):\n",
        "        params = dict(\n",
        "        n_estimators= [100,150],\n",
        "        criterion = ['gini', 'entropy'],\n",
        "        min_samples_split = [2, 3, 4],\n",
        "        min_samples_leaf = [1, 2, 3, 4, 5],\n",
        "        max_features = ['auto', 'sqrt'],\n",
        "        max_depth = [5, 6, 7]\n",
        "        )\n",
        "\n",
        "    elif type(model) == type(DecisionTreeClassifier()):\n",
        "        params = dict(\n",
        "        criterion = ['gini', 'entropy'],\n",
        "        min_samples_split = [2, 3, 4],\n",
        "        min_samples_leaf = [1, 2, 3, 4, 5],\n",
        "        max_depth = [5, 6, 7])\n",
        "\n",
        "\n",
        "\n",
        "    elif type(model) == type(xgboost.XGBClassifier()):\n",
        "        params = dict(gamma = [0.1, 0.5, 2, 6, 25, 100],\n",
        "                      learning_rate = [0.01, 0.03, 0.06, 0.1, 0.2, 0.3],\n",
        "                      max_depth = [5, 10, 15, 20],\n",
        "                      n_estimators = [50, 80, 115, 150],\n",
        "                      reg_alpha = [0.1, 0.5, 2, 6, 25, 100],\n",
        "                      reg_lambda = [0.1, 0.5, 2, 6, 25, 100])\n",
        "\n",
        "\n",
        "\n",
        "      # define evaluation\n",
        "    cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
        "\n",
        "\n",
        "    #random search\n",
        "\n",
        "    search = RandomizedSearchCV(model, params,  n_iter=100, scoring='accuracy', n_jobs=-1, cv=cv)\n",
        "\n",
        "\n",
        "    # execute search\n",
        "    result = search.fit(X, Y)\n",
        "    print('SEARCH COMPLETE')\n",
        "\n",
        "\n",
        "\n",
        "    # return the best result\n",
        "    return result.best_score_ , result.best_params_\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2Oozv_QEzDK"
      },
      "outputs": [],
      "source": [
        " def decision_classifier(X_train,y_train):\n",
        "    params={\n",
        "    'min_samples_split':3,\n",
        "    'min_samples_leaf':3,\n",
        "    'max_depth':6,\n",
        "    'criterion':'gini'}\n",
        "\n",
        "    w = {0:9, 1:91}\n",
        "\n",
        "\n",
        "    dt = DecisionTreeClassifier(**params, class_weight = w)\n",
        "    dt.fit(X_train, y_train)\n",
        "    dt_preds = dt.predict(X_test)\n",
        "\n",
        "    print(\"Accuracy TEST SET:\",metrics.accuracy_score(y_test, dt_preds))\n",
        "\n",
        "\n",
        "    print(\"The precision, recall and f1 score:\", precision_recall_fscore_support(y_test, dt_preds, average='binary'))\n",
        "    print('The balanced accuracy:',balanced_accuracy_score(y_test, dt_preds))\n",
        "\n",
        "decision_classifier(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW5UtwyIMs2E"
      },
      "outputs": [],
      "source": [
        " #sum of weight for the xgboost classifier, he does not have the class weight\n",
        "y_train_0 = []\n",
        "y_train_1 = []\n",
        "for i in y_train:\n",
        "    if i == 0:\n",
        "        y_train_0.append(i)\n",
        "    elif i == 1:\n",
        "        y_train_1.append(i)\n",
        "\n",
        "weight = len(y_train_0) / len(y_train_1)\n",
        "weight\n",
        "\n",
        "def xg_boost(X_train,y_train):\n",
        "    params = params={\n",
        "    'reg_lambda':25,\n",
        "    'reg_alpha':2,\n",
        "    'n_estimators':115,\n",
        "    'max_depth': 20,\n",
        "    'learning_rate':0.1,\n",
        "    'gamma':6}\n",
        "\n",
        "\n",
        "    xg = XGBClassifier(**params, scale_pos_weight=weight) # Init model\n",
        "    xg.fit(X_train, y_train) # Fit model\n",
        "    xg_preds = xg.predict(X_test.astype(int)) # Get model inference\n",
        "\n",
        "    print(\"Accuracy TEST SET:\",metrics.accuracy_score(y_test, xg_preds))\n",
        "\n",
        "\n",
        "    print(\"The precision, recall and f1 score:\", precision_recall_fscore_support(y_test, xg_preds, average='binary'))\n",
        "    print('The balanced accuracy:',balanced_accuracy_score(y_test, xg_preds))\n",
        "\n",
        "xg_boost(X_train.astype(int),y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkZajh86R3EA"
      },
      "outputs": [],
      "source": [
        "def random_forest(X_train,y_train):\n",
        "  params = {'n_estimators': 150,\n",
        "  'min_samples_split': 2,\n",
        "  'min_samples_leaf': 4,\n",
        "  'max_features': 'auto',\n",
        "  'max_depth': 7,\n",
        "  'criterion': 'gini'}\n",
        "\n",
        "  w = {0:9, 1:91}\n",
        "\n",
        "\n",
        "  rf = RandomForestClassifier(**params, class_weight = w)\n",
        "  rf.fit(X_train, y_train)\n",
        "  rf_preds = rf.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy TEST SET:\",metrics.accuracy_score(y_test, rf_preds))\n",
        "\n",
        "\n",
        "  print(\"The precision, recall and f1 score:\", precision_recall_fscore_support(y_test, rf_preds, average='binary'))\n",
        "  print('The balanced accuracy:',balanced_accuracy_score(y_test, rf_preds))\n",
        "\n",
        "random_forest(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC CURVE FOR ALL THE MACHINE LEARNING MODELS\n"
      ],
      "metadata": {
        "id": "Ds8oUo_ozbAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC Curve for every trained model, and adjust the decision treshold\n",
        "\n",
        "#ROC curve for all the trained models\n",
        "params={\n",
        "    'min_samples_split':3,\n",
        "    'min_samples_leaf':3,\n",
        "    'max_depth':6,\n",
        "    'criterion':'gini'}\n",
        "\n",
        "w = {0:9, 1:91}\n",
        "\n",
        "\n",
        "dt = DecisionTreeClassifier(**params, class_weight = w)\n",
        "dt.fit(X_train, y_train)\n",
        "probs_dt = dt.predict_proba(X_test)[:, 1]\n"
      ],
      "metadata": {
        "id": "a3ruNrlW0qJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'n_estimators': 150,\n",
        "  'min_samples_split': 2,\n",
        "  'min_samples_leaf': 4,\n",
        "  'max_features': 'auto',\n",
        "  'max_depth': 7,\n",
        "  'criterion': 'gini'}\n",
        "\n",
        "w = {0:9, 1:91}\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(**params, class_weight = w)\n",
        "rf.fit(X_train, y_train)\n",
        "probs_rf = rf.predict_proba(X_test)[:,1]\n",
        ""
      ],
      "metadata": {
        "id": "bJWAX9NHzfva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params={\n",
        "    'reg_lambda':25,\n",
        "    'reg_alpha':2,\n",
        "    'n_estimators':115,\n",
        "    'max_depth': 20,\n",
        "    'learning_rate':0.1,\n",
        "    'gamma':6}\n",
        "\n",
        "\n",
        "#scale_pos_ = 10.594500632111252\n",
        "xg = XGBClassifier(**params, scale_pos_weight=weight) # Init model\n",
        "xg.fit(X_train.astype(int), y_train) # Fit model\n",
        "probs_xg = xg.predict_proba(X_test.astype(int))[:,1] # Get model inference"
      ],
      "metadata": {
        "id": "CPw5U6V00iOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = {0:9, 1:91}\n",
        "lr = LogisticRegression(random_state=random_state, class_weight=w)\n",
        "lr.fit(X_train, y_train)\n",
        "probs_lr = lr.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "PUppYeug0lsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc_lr = roc_auc_score(y_test, probs_lr)\n",
        "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, probs_lr)\n",
        "\n",
        "auc_dt = roc_auc_score(y_test, probs_dt)\n",
        "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, probs_dt)\n",
        "\n",
        "auc_rf = roc_auc_score(y_test, probs_rf)\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, probs_rf)\n",
        "\n",
        "auc_xg = roc_auc_score(y_test, probs_xg)\n",
        "fpr_xg, tpr_xg, thresholds_xg = roc_curve(y_test, probs_xg)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'AUC (Logistic Regression) = {auc_lr:.2f}')\n",
        "plt.plot(fpr_dt, tpr_dt, label=f'AUC (Decision Tree) = {auc_dt:.2f}')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'AUC (Random Forests) = {auc_rf:.2f}')\n",
        "plt.plot(fpr_xg, tpr_xg, label=f'AUC (XGBoost) = {auc_xg:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='blue', linestyle='--', label='Baseline')\n",
        "plt.title('ROC Curve', size=20)\n",
        "plt.xlabel('False Positive Rate', size=14)\n",
        "plt.ylabel('True Positive Rate', size=14)\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "d6Q1pRSB0s3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the decision treshold based on our problem statement"
      ],
      "metadata": {
        "id": "WsE1KmY66fC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#adjust the tresholds for all the trained models, based on the ROC curves\n",
        "# calculate roc curves\n",
        "\n",
        "optimial_dt = np.argmax(tpr_dt - fpr_dt)\n",
        "\n",
        "best_thresh_dt = thresholds_dt[optimial_dt]\n",
        "\n",
        "print('Best Threshold=%f for decision tree' % (best_thresh_dt))\n",
        "\n",
        "# get the best threshold for logistic regression\n",
        "J_lr = tpr_lr - fpr_lr\n",
        "ix_lr = argmax(J_lr)\n",
        "best_thresh_lr = thresholds_lr[ix_lr]\n",
        "print('Best Threshold=%f for logistic regression' % (best_thresh_lr))\n",
        "\n",
        "\n",
        "# get the best threshold for xgboost\n",
        "J_xg = tpr_xg - fpr_xg\n",
        "ix_xg = argmax(J_xg)\n",
        "best_thresh_xg = thresholds_xg[ix_xg]\n",
        "print('Best Threshold=%f for xgboost' % (best_thresh_xg))\n",
        "\n",
        "\n",
        "\n",
        "# get the best threshold for random forest\n",
        "J_rf = tpr_rf - fpr_rf\n",
        "ix_rf = argmax(J_rf)\n",
        "best_thresh_rf = thresholds_rf[ix_rf]\n",
        "print('Best Threshold=%f for random forest' % (best_thresh_rf))"
      ],
      "metadata": {
        "id": "5HqvGWxA0vgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs_xg = xg.predict_proba(X_val.astype(int))\n",
        "probs_xg_1 = probs_xg[:, 1]\n",
        "probs_xg_new = (probs_xg_1 >= 0.172782).astype(int)\n",
        "\n",
        "print(\"Accuracy TEST SET:\",metrics.accuracy_score(y_val, probs_xg_new))\n",
        "print(\"The precision, recall and f1 score:\", precision_recall_fscore_support(y_val, probs_xg_new, average='binary'))\n",
        "print('The balanced accuracy:',balanced_accuracy_score(y_val, probs_xg_new))"
      ],
      "metadata": {
        "id": "bc9e6jk-560M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}